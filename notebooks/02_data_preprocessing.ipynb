{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efccebb",
   "metadata": {},
   "source": [
    "# üîÑ Data Preprocessing for SentinelGem\n",
    "\n",
    "**Author:** Muzan Sano  \n",
    "**Purpose:** Process real cybersecurity datasets for SentinelGem training and validation\n",
    "\n",
    "This notebook processes the downloaded datasets and prepares them for integration with SentinelGem's multimodal AI analysis pipeline.\n",
    "\n",
    "### üéØ Processing Goals\n",
    "- **üìß Clean phishing/spam email data** for text analysis training\n",
    "- **üîó Process malicious URL datasets** for pattern recognition\n",
    "- **üì∑ Prepare phishing screenshot data** for OCR/visual analysis\n",
    "- **üé§ Convert audio datasets** for social engineering detection\n",
    "- **üìã Parse system/network logs** for malware signature extraction\n",
    "- **‚öôÔ∏è Update SentinelGem rules** with real-world threat patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060655b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import zipfile\n",
    "import shutil\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"üîÑ SentinelGem Data Preprocessing Started\")\n",
    "print(f\"üìÖ Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìÇ Project Root: {project_root}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f07ef",
   "metadata": {},
   "source": [
    "## üìÅ Dataset Discovery and Extraction\n",
    "\n",
    "First, let's discover and extract any downloaded datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "datasets_dir = project_root / \"assets\" / \"datasets\"\n",
    "downloads_dir = Path.home() / \".kaggle\" / \"datasets\"  # Common Kaggle download location\n",
    "\n",
    "def discover_datasets():\n",
    "    \"\"\"Discover downloaded datasets in various locations\"\"\"\n",
    "    discovered = []\n",
    "    \n",
    "    # Check common download locations\n",
    "    search_paths = [\n",
    "        datasets_dir,\n",
    "        downloads_dir,\n",
    "        Path.cwd(),  # Current directory\n",
    "        Path.home() / \"Downloads\"  # User downloads\n",
    "    ]\n",
    "    \n",
    "    for search_path in search_paths:\n",
    "        if search_path.exists():\n",
    "            # Look for zip files\n",
    "            for zip_file in search_path.glob(\"**/*.zip\"):\n",
    "                discovered.append({\n",
    "                    \"path\": zip_file,\n",
    "                    \"name\": zip_file.stem,\n",
    "                    \"size\": zip_file.stat().st_size,\n",
    "                    \"type\": \"zip\"\n",
    "                })\n",
    "            \n",
    "            # Look for CSV files (common for Kaggle datasets)\n",
    "            for csv_file in search_path.glob(\"**/*.csv\"):\n",
    "                if csv_file.stat().st_size > 1000:  # Ignore small files\n",
    "                    discovered.append({\n",
    "                        \"path\": csv_file,\n",
    "                        \"name\": csv_file.stem,\n",
    "                        \"size\": csv_file.stat().st_size,\n",
    "                        \"type\": \"csv\"\n",
    "                    })\n",
    "    \n",
    "    return discovered\n",
    "\n",
    "def extract_datasets(discovered_datasets):\n",
    "    \"\"\"Extract zip datasets to organized structure\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    for dataset in discovered_datasets:\n",
    "        if dataset[\"type\"] == \"zip\":\n",
    "            # Determine extraction path based on dataset name\n",
    "            dataset_name = dataset[\"name\"].lower()\n",
    "            \n",
    "            if \"phishing\" in dataset_name and \"url\" in dataset_name:\n",
    "                extract_path = datasets_dir / \"phishing\" / \"urls\"\n",
    "            elif \"phishing\" in dataset_name and \"screenshot\" in dataset_name:\n",
    "                extract_path = datasets_dir / \"images\" / \"phishing_sites\"\n",
    "            elif \"spam\" in dataset_name or \"email\" in dataset_name:\n",
    "                extract_path = datasets_dir / \"phishing\" / \"emails\"\n",
    "            elif \"malware\" in dataset_name:\n",
    "                extract_path = datasets_dir / \"malware\" / \"samples\"\n",
    "            elif \"audio\" in dataset_name or \"speech\" in dataset_name:\n",
    "                extract_path = datasets_dir / \"audio\" / \"voice_samples\"\n",
    "            elif \"network\" in dataset_name or \"intrusion\" in dataset_name:\n",
    "                extract_path = datasets_dir / \"logs\" / \"network_logs\"\n",
    "            else:\n",
    "                extract_path = datasets_dir / \"processed\" / dataset_name\n",
    "            \n",
    "            # Create extraction directory\n",
    "            extract_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            try:\n",
    "                with zipfile.ZipFile(dataset[\"path\"], 'r') as zip_ref:\n",
    "                    zip_ref.extractall(extract_path)\n",
    "                    extracted.append({\n",
    "                        \"original\": dataset[\"path\"],\n",
    "                        \"extracted_to\": extract_path,\n",
    "                        \"name\": dataset[\"name\"]\n",
    "                    })\n",
    "                    print(f\"‚úÖ Extracted {dataset['name']} to {extract_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to extract {dataset['name']}: {e}\")\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "# Discover and extract datasets\n",
    "print(\"üîç Discovering downloaded datasets...\")\n",
    "discovered = discover_datasets()\n",
    "\n",
    "print(f\"\\nüìä Found {len(discovered)} potential datasets:\")\n",
    "for dataset in discovered:\n",
    "    size_mb = dataset[\"size\"] / (1024 * 1024)\n",
    "    print(f\"  üìÅ {dataset['name']} ({dataset['type']}) - {size_mb:.1f}MB\")\n",
    "\n",
    "if discovered:\n",
    "    print(\"\\nüîì Extracting datasets...\")\n",
    "    extracted = extract_datasets(discovered)\n",
    "    print(f\"\\n‚úÖ Successfully extracted {len(extracted)} datasets\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No datasets found. Please download datasets using:\")\n",
    "    print(\"   kaggle datasets download -d <dataset-name>\")\n",
    "    extracted = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027f8bd",
   "metadata": {},
   "source": [
    "## üìß Phishing Email Processing\n",
    "\n",
    "Process phishing and spam email datasets for text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2494a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_email_datasets():\n",
    "    \"\"\"Process email/spam datasets for phishing detection\"\"\"\n",
    "    email_dir = datasets_dir / \"phishing\" / \"emails\"\n",
    "    processed_data = []\n",
    "    \n",
    "    if not email_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Email dataset directory not found: {email_dir}\")\n",
    "        return None\n",
    "    \n",
    "    # Look for CSV files in email directory\n",
    "    csv_files = list(email_dir.glob(\"**/*.csv\"))\n",
    "    \n",
    "    print(f\"üìß Processing {len(csv_files)} email dataset files...\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            # Try different encodings\n",
    "            for encoding in ['utf-8', 'latin-1', 'iso-8859-1']:\n",
    "                try:\n",
    "                    df = pd.read_csv(csv_file, encoding=encoding)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"‚ùå Could not read {csv_file.name} with any encoding\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"\\nüìÅ Processing {csv_file.name}:\")\n",
    "            print(f\"   üìä Shape: {df.shape}\")\n",
    "            print(f\"   üìã Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Try to identify text and label columns\n",
    "            text_col = None\n",
    "            label_col = None\n",
    "            \n",
    "            # Common column names for text\n",
    "            text_candidates = ['text', 'email', 'message', 'body', 'content', 'subject']\n",
    "            for col in df.columns:\n",
    "                if col.lower() in text_candidates:\n",
    "                    text_col = col\n",
    "                    break\n",
    "            \n",
    "            # Common column names for labels\n",
    "            label_candidates = ['label', 'class', 'category', 'type', 'spam', 'ham']\n",
    "            for col in df.columns:\n",
    "                if col.lower() in label_candidates:\n",
    "                    label_col = col\n",
    "                    break\n",
    "            \n",
    "            if text_col and label_col:\n",
    "                # Extract phishing patterns\n",
    "                phishing_samples = df[df[label_col].astype(str).str.lower().str.contains('spam|phish|malicious', na=False)]\n",
    "                legitimate_samples = df[~df[label_col].astype(str).str.lower().str.contains('spam|phish|malicious', na=False)]\n",
    "                \n",
    "                print(f\"   üéØ Phishing samples: {len(phishing_samples)}\")\n",
    "                print(f\"   ‚úÖ Legitimate samples: {len(legitimate_samples)}\")\n",
    "                \n",
    "                # Extract common phishing patterns\n",
    "                if len(phishing_samples) > 0:\n",
    "                    phishing_text = ' '.join(phishing_samples[text_col].astype(str).tolist())\n",
    "                    patterns = extract_phishing_patterns(phishing_text)\n",
    "                    \n",
    "                    processed_data.append({\n",
    "                        'file': csv_file.name,\n",
    "                        'phishing_count': len(phishing_samples),\n",
    "                        'legitimate_count': len(legitimate_samples),\n",
    "                        'patterns': patterns,\n",
    "                        'samples': phishing_samples[text_col].head(5).tolist()\n",
    "                    })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {csv_file.name}: {e}\")\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def extract_phishing_patterns(text):\n",
    "    \"\"\"Extract common phishing patterns from text\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    patterns = {\n",
    "        'urgency_words': [],\n",
    "        'credential_requests': [],\n",
    "        'suspicious_domains': [],\n",
    "        'action_phrases': []\n",
    "    }\n",
    "    \n",
    "    # Urgency indicators\n",
    "    urgency_regex = r'\\b(urgent|immediate|expire|suspend|limited time|act now|final notice)\\b'\n",
    "    patterns['urgency_words'] = list(set(re.findall(urgency_regex, text_lower)))\n",
    "    \n",
    "    # Credential requests\n",
    "    cred_regex = r'\\b(password|username|login|credit card|bank account|social security|pin|verify)\\b'\n",
    "    patterns['credential_requests'] = list(set(re.findall(cred_regex, text_lower)))\n",
    "    \n",
    "    # Extract URLs and domains\n",
    "    url_regex = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    urls = re.findall(url_regex, text)\n",
    "    domains = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            domain = urlparse(url).netloc\n",
    "            if domain:\n",
    "                domains.append(domain)\n",
    "        except:\n",
    "            pass\n",
    "    patterns['suspicious_domains'] = list(set(domains))\n",
    "    \n",
    "    # Action phrases\n",
    "    action_regex = r'\\b(click here|download|update|verify|confirm|claim|redeem)\\b'\n",
    "    patterns['action_phrases'] = list(set(re.findall(action_regex, text_lower)))\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "# Process email datasets\n",
    "print(\"üìß PHISHING EMAIL PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "email_data = process_email_datasets()\n",
    "\n",
    "if email_data:\n",
    "    print(f\"\\n‚úÖ Processed {len(email_data)} email datasets\")\n",
    "    \n",
    "    # Combine all patterns\n",
    "    combined_patterns = {\n",
    "        'urgency_words': set(),\n",
    "        'credential_requests': set(),\n",
    "        'suspicious_domains': set(),\n",
    "        'action_phrases': set()\n",
    "    }\n",
    "    \n",
    "    for data in email_data:\n",
    "        for category, patterns in data['patterns'].items():\n",
    "            combined_patterns[category].update(patterns)\n",
    "    \n",
    "    print(\"\\nüéØ Extracted Phishing Patterns:\")\n",
    "    for category, patterns in combined_patterns.items():\n",
    "        print(f\"   {category}: {len(patterns)} unique patterns\")\n",
    "        if patterns:\n",
    "            print(f\"      Examples: {list(patterns)[:5]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No email datasets processed. Using existing patterns.\")\n",
    "    combined_patterns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df310dad",
   "metadata": {},
   "source": [
    "## üîó URL Dataset Processing\n",
    "\n",
    "Process malicious URL datasets for pattern recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784a9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url_datasets():\n",
    "    \"\"\"Process URL datasets for phishing detection\"\"\"\n",
    "    url_dir = datasets_dir / \"phishing\" / \"urls\"\n",
    "    processed_urls = []\n",
    "    \n",
    "    if not url_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è URL dataset directory not found: {url_dir}\")\n",
    "        return None\n",
    "    \n",
    "    csv_files = list(url_dir.glob(\"**/*.csv\"))\n",
    "    print(f\"üîó Processing {len(csv_files)} URL dataset files...\")\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "            print(f\"\\nüìÅ Processing {csv_file.name}:\")\n",
    "            print(f\"   üìä Shape: {df.shape}\")\n",
    "            print(f\"   üìã Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Find URL and label columns\n",
    "            url_col = None\n",
    "            label_col = None\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if 'url' in col.lower() or 'link' in col.lower():\n",
    "                    url_col = col\n",
    "                if 'label' in col.lower() or 'class' in col.lower() or 'type' in col.lower():\n",
    "                    label_col = col\n",
    "            \n",
    "            if url_col:\n",
    "                # Analyze URL patterns\n",
    "                urls = df[url_col].dropna().astype(str).tolist()\n",
    "                malicious_urls = []\n",
    "                legitimate_urls = []\n",
    "                \n",
    "                if label_col:\n",
    "                    malicious_mask = df[label_col].astype(str).str.lower().str.contains('bad|malicious|phish|spam', na=False)\n",
    "                    malicious_urls = df[malicious_mask][url_col].dropna().astype(str).tolist()\n",
    "                    legitimate_urls = df[~malicious_mask][url_col].dropna().astype(str).tolist()\n",
    "                \n",
    "                # Extract URL features\n",
    "                url_features = analyze_url_patterns(malicious_urls if malicious_urls else urls)\n",
    "                \n",
    "                processed_urls.append({\n",
    "                    'file': csv_file.name,\n",
    "                    'total_urls': len(urls),\n",
    "                    'malicious_urls': len(malicious_urls),\n",
    "                    'legitimate_urls': len(legitimate_urls),\n",
    "                    'features': url_features\n",
    "                })\n",
    "                \n",
    "                print(f\"   üéØ Malicious URLs: {len(malicious_urls)}\")\n",
    "                print(f\"   ‚úÖ Legitimate URLs: {len(legitimate_urls)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {csv_file.name}: {e}\")\n",
    "    \n",
    "    return processed_urls\n",
    "\n",
    "def analyze_url_patterns(urls):\n",
    "    \"\"\"Analyze patterns in URLs\"\"\"\n",
    "    features = {\n",
    "        'suspicious_domains': set(),\n",
    "        'short_urls': set(),\n",
    "        'ip_addresses': [],\n",
    "        'suspicious_paths': set(),\n",
    "        'long_domains': []\n",
    "    }\n",
    "    \n",
    "    for url in urls[:1000]:  # Limit to first 1000 for performance\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            domain = parsed.netloc.lower()\n",
    "            path = parsed.path.lower()\n",
    "            \n",
    "            # Check for IP addresses\n",
    "            ip_pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b'\n",
    "            if re.match(ip_pattern, domain):\n",
    "                features['ip_addresses'].append(domain)\n",
    "            \n",
    "            # Check for short URL services\n",
    "            short_services = ['bit.ly', 'tinyurl.com', 't.co', 'goo.gl', 'ow.ly']\n",
    "            if any(service in domain for service in short_services):\n",
    "                features['short_urls'].add(domain)\n",
    "            \n",
    "            # Check for suspicious domains\n",
    "            suspicious_keywords = ['secure', 'verification', 'update', 'confirm', 'account']\n",
    "            if any(keyword in domain for keyword in suspicious_keywords):\n",
    "                features['suspicious_domains'].add(domain)\n",
    "            \n",
    "            # Check for long domains (possible typosquatting)\n",
    "            if len(domain) > 30:\n",
    "                features['long_domains'].append(domain)\n",
    "            \n",
    "            # Check for suspicious paths\n",
    "            suspicious_path_keywords = ['login', 'verify', 'update', 'secure', 'account']\n",
    "            if any(keyword in path for keyword in suspicious_path_keywords):\n",
    "                features['suspicious_paths'].add(path)\n",
    "                \n",
    "        except Exception:\n",
    "            continue\n",
    "    \n",
    "    # Convert sets to lists for JSON serialization\n",
    "    for key in features:\n",
    "        if isinstance(features[key], set):\n",
    "            features[key] = list(features[key])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Process URL datasets\n",
    "print(\"\\nüîó URL DATASET PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "url_data = process_url_datasets()\n",
    "\n",
    "if url_data:\n",
    "    print(f\"\\n‚úÖ Processed {len(url_data)} URL datasets\")\n",
    "    \n",
    "    # Combine URL features\n",
    "    combined_url_features = {\n",
    "        'suspicious_domains': set(),\n",
    "        'short_urls': set(),\n",
    "        'ip_addresses': set(),\n",
    "        'suspicious_paths': set()\n",
    "    }\n",
    "    \n",
    "    for data in url_data:\n",
    "        for category, items in data['features'].items():\n",
    "            if category in combined_url_features:\n",
    "                combined_url_features[category].update(items)\n",
    "    \n",
    "    print(\"\\nüéØ Extracted URL Patterns:\")\n",
    "    for category, items in combined_url_features.items():\n",
    "        print(f\"   {category}: {len(items)} unique patterns\")\n",
    "        if items:\n",
    "            print(f\"      Examples: {list(items)[:3]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No URL datasets processed. Using existing patterns.\")\n",
    "    combined_url_features = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef96256",
   "metadata": {},
   "source": [
    "## üìã Malware Log Processing\n",
    "\n",
    "Process malware and system log datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4355cfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_malware_datasets():\n",
    "    \"\"\"Process malware and system log datasets\"\"\"\n",
    "    malware_dir = datasets_dir / \"malware\"\n",
    "    network_dir = datasets_dir / \"logs\" / \"network_logs\"\n",
    "    processed_malware = []\n",
    "    \n",
    "    # Process both malware and network log directories\n",
    "    search_dirs = [malware_dir, network_dir]\n",
    "    \n",
    "    for search_dir in search_dirs:\n",
    "        if not search_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        csv_files = list(search_dir.glob(\"**/*.csv\"))\n",
    "        print(f\"üìã Processing {len(csv_files)} files from {search_dir.name}...\")\n",
    "        \n",
    "        for csv_file in csv_files:\n",
    "            try:\n",
    "                # Read dataset\n",
    "                df = pd.read_csv(csv_file, encoding='utf-8', nrows=10000)  # Limit rows for performance\n",
    "                print(f\"\\nüìÅ Processing {csv_file.name}:\")\n",
    "                print(f\"   üìä Shape: {df.shape}\")\n",
    "                print(f\"   üìã Columns: {list(df.columns)[:10]}...\")  # Show first 10 columns\n",
    "                \n",
    "                # Analyze for malware indicators\n",
    "                malware_indicators = extract_malware_indicators(df)\n",
    "                \n",
    "                processed_malware.append({\n",
    "                    'file': csv_file.name,\n",
    "                    'source_dir': search_dir.name,\n",
    "                    'shape': df.shape,\n",
    "                    'indicators': malware_indicators\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {csv_file.name}: {e}\")\n",
    "    \n",
    "    return processed_malware\n",
    "\n",
    "def extract_malware_indicators(df):\n",
    "    \"\"\"Extract malware indicators from dataset\"\"\"\n",
    "    indicators = {\n",
    "        'suspicious_processes': set(),\n",
    "        'network_patterns': set(),\n",
    "        'file_patterns': set(),\n",
    "        'registry_patterns': set()\n",
    "    }\n",
    "    \n",
    "    # Convert all columns to string for pattern matching\n",
    "    text_data = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # String columns\n",
    "            text_data.extend(df[col].astype(str).tolist())\n",
    "    \n",
    "    combined_text = ' '.join(text_data).lower()\n",
    "    \n",
    "    # Extract suspicious processes\n",
    "    process_patterns = [\n",
    "        r'powershell.*-enc', r'cmd.*\\/c', r'regsvr32', r'rundll32',\n",
    "        r'bitsadmin', r'certutil', r'wscript', r'cscript'\n",
    "    ]\n",
    "    for pattern in process_patterns:\n",
    "        matches = re.findall(pattern, combined_text)\n",
    "        indicators['suspicious_processes'].update(matches)\n",
    "    \n",
    "    # Extract network patterns\n",
    "    network_patterns = [\n",
    "        r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}:[0-9]+\\b',  # IP:Port\n",
    "        r'http://[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+',  # HTTP IP addresses\n",
    "        r'tcp://.*:[0-9]{4,5}'  # TCP connections\n",
    "    ]\n",
    "    for pattern in network_patterns:\n",
    "        matches = re.findall(pattern, combined_text)\n",
    "        indicators['network_patterns'].update(matches)\n",
    "    \n",
    "    # Extract file patterns\n",
    "    file_patterns = [\n",
    "        r'\\\\temp\\\\.*\\.exe', r'\\\\appdata\\\\.*\\.exe',\n",
    "        r'.*\\.scr', r'.*\\.bat', r'.*\\.vbs'\n",
    "    ]\n",
    "    for pattern in file_patterns:\n",
    "        matches = re.findall(pattern, combined_text, re.IGNORECASE)\n",
    "        indicators['file_patterns'].update(matches)\n",
    "    \n",
    "    # Extract registry patterns\n",
    "    registry_patterns = [\n",
    "        r'hklm\\\\.*\\\\run', r'hkcu\\\\.*\\\\run',\n",
    "        r'hklm\\\\software\\\\microsoft\\\\windows\\\\currentversion'\n",
    "    ]\n",
    "    for pattern in registry_patterns:\n",
    "        matches = re.findall(pattern, combined_text, re.IGNORECASE)\n",
    "        indicators['registry_patterns'].update(matches)\n",
    "    \n",
    "    # Convert sets to lists\n",
    "    for key in indicators:\n",
    "        indicators[key] = list(indicators[key])[:10]  # Limit to 10 examples\n",
    "    \n",
    "    return indicators\n",
    "\n",
    "# Process malware datasets\n",
    "print(\"\\nüìã MALWARE DATASET PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "malware_data = process_malware_datasets()\n",
    "\n",
    "if malware_data:\n",
    "    print(f\"\\n‚úÖ Processed {len(malware_data)} malware datasets\")\n",
    "    \n",
    "    # Combine malware indicators\n",
    "    combined_malware_indicators = {\n",
    "        'suspicious_processes': set(),\n",
    "        'network_patterns': set(),\n",
    "        'file_patterns': set(),\n",
    "        'registry_patterns': set()\n",
    "    }\n",
    "    \n",
    "    for data in malware_data:\n",
    "        for category, patterns in data['indicators'].items():\n",
    "            combined_malware_indicators[category].update(patterns)\n",
    "    \n",
    "    print(\"\\nüéØ Extracted Malware Indicators:\")\n",
    "    for category, patterns in combined_malware_indicators.items():\n",
    "        print(f\"   {category}: {len(patterns)} unique patterns\")\n",
    "        if patterns:\n",
    "            print(f\"      Examples: {list(patterns)[:3]}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No malware datasets processed. Using existing patterns.\")\n",
    "    combined_malware_indicators = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54a19e",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Update SentinelGem Rules\n",
    "\n",
    "Update the threat detection rules with patterns extracted from real data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b39ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_threat_rules(email_patterns=None, url_features=None, malware_indicators=None):\n",
    "    \"\"\"Update SentinelGem threat detection rules with real data patterns\"\"\"\n",
    "    rules_file = project_root / \"config\" / \"rules.yaml\"\n",
    "    \n",
    "    # Load existing rules\n",
    "    try:\n",
    "        with open(rules_file, 'r') as f:\n",
    "            rules = yaml.safe_load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading rules: {e}\")\n",
    "        return False\n",
    "    \n",
    "    updated = False\n",
    "    \n",
    "    # Update phishing rules with email patterns\n",
    "    if email_patterns:\n",
    "        if 'phishing' not in rules:\n",
    "            rules['phishing'] = {}\n",
    "        \n",
    "        # Add new urgency indicators\n",
    "        if email_patterns.get('urgency_words'):\n",
    "            existing_urgency = set(rules['phishing'].get('urgency_indicators', []))\n",
    "            new_urgency = existing_urgency.union(email_patterns['urgency_words'])\n",
    "            rules['phishing']['urgency_indicators'] = list(new_urgency)\n",
    "            updated = True\n",
    "            print(f\"   ‚úÖ Added {len(email_patterns['urgency_words'])} urgency patterns\")\n",
    "        \n",
    "        # Add credential harvesting patterns\n",
    "        if email_patterns.get('credential_requests'):\n",
    "            existing_creds = set(rules['phishing'].get('credential_harvesting', []))\n",
    "            new_creds = existing_creds.union(email_patterns['credential_requests'])\n",
    "            rules['phishing']['credential_harvesting'] = list(new_creds)\n",
    "            updated = True\n",
    "            print(f\"   ‚úÖ Added {len(email_patterns['credential_requests'])} credential patterns\")\n",
    "        \n",
    "        # Add suspicious domains\n",
    "        if email_patterns.get('suspicious_domains'):\n",
    "            existing_domains = set(rules['phishing'].get('suspicious_domains', []))\n",
    "            new_domains = existing_domains.union(email_patterns['suspicious_domains'])\n",
    "            rules['phishing']['suspicious_domains'] = list(new_domains)\n",
    "            updated = True\n",
    "            print(f\"   ‚úÖ Added {len(email_patterns['suspicious_domains'])} domain patterns\")\n",
    "    \n",
    "    # Update URL patterns\n",
    "    if url_features:\n",
    "        if 'url_analysis' not in rules:\n",
    "            rules['url_analysis'] = {}\n",
    "        \n",
    "        for category, patterns in url_features.items():\n",
    "            if patterns:\n",
    "                existing_patterns = set(rules['url_analysis'].get(category, []))\n",
    "                new_patterns = existing_patterns.union(patterns)\n",
    "                rules['url_analysis'][category] = list(new_patterns)\n",
    "                updated = True\n",
    "                print(f\"   ‚úÖ Added {len(patterns)} {category} patterns\")\n",
    "    \n",
    "    # Update malware indicators\n",
    "    if malware_indicators:\n",
    "        if 'malware' not in rules:\n",
    "            rules['malware'] = {}\n",
    "        \n",
    "        for category, patterns in malware_indicators.items():\n",
    "            if patterns:\n",
    "                existing_patterns = set(rules['malware'].get(category, []))\n",
    "                new_patterns = existing_patterns.union(patterns)\n",
    "                rules['malware'][category] = list(new_patterns)\n",
    "                updated = True\n",
    "                print(f\"   ‚úÖ Added {len(patterns)} {category} patterns\")\n",
    "    \n",
    "    # Save updated rules\n",
    "    if updated:\n",
    "        try:\n",
    "            # Create backup\n",
    "            backup_file = rules_file.with_suffix('.yaml.backup')\n",
    "            shutil.copy2(rules_file, backup_file)\n",
    "            \n",
    "            # Save updated rules\n",
    "            with open(rules_file, 'w') as f:\n",
    "                yaml.dump(rules, f, default_flow_style=False, indent=2)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Rules updated successfully!\")\n",
    "            print(f\"   üìÅ Updated: {rules_file}\")\n",
    "            print(f\"   üíæ Backup: {backup_file}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error saving rules: {e}\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è No new patterns to add to rules\")\n",
    "        return False\n",
    "\n",
    "# Update threat detection rules\n",
    "print(\"\\n‚öôÔ∏è UPDATING SENTINELGEM RULES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert sets to lists for rule update\n",
    "email_patterns_for_rules = None\n",
    "if combined_patterns:\n",
    "    email_patterns_for_rules = {k: list(v) for k, v in combined_patterns.items()}\n",
    "\n",
    "url_features_for_rules = None\n",
    "if combined_url_features:\n",
    "    url_features_for_rules = {k: list(v) for k, v in combined_url_features.items()}\n",
    "\n",
    "malware_indicators_for_rules = None\n",
    "if combined_malware_indicators:\n",
    "    malware_indicators_for_rules = {k: list(v) for k, v in combined_malware_indicators.items()}\n",
    "\n",
    "success = update_threat_rules(\n",
    "    email_patterns_for_rules,\n",
    "    url_features_for_rules,\n",
    "    malware_indicators_for_rules\n",
    ")\n",
    "\n",
    "if success:\n",
    "    print(\"\\nüéØ Rule update completed successfully!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No rule updates were made\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6607fbb",
   "metadata": {},
   "source": [
    "## üìä Generate Training/Testing Splits\n",
    "\n",
    "Create organized training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027c60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_splits():\n",
    "    \"\"\"Create training/validation/testing splits from processed data\"\"\"\n",
    "    processed_dir = datasets_dir / \"processed\"\n",
    "    processed_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    splits = {\n",
    "        'training': processed_dir / 'training',\n",
    "        'validation': processed_dir / 'validation', \n",
    "        'testing': processed_dir / 'testing'\n",
    "    }\n",
    "    \n",
    "    # Create split directories\n",
    "    for split_name, split_dir in splits.items():\n",
    "        split_dir.mkdir(exist_ok=True)\n",
    "        (split_dir / 'phishing').mkdir(exist_ok=True)\n",
    "        (split_dir / 'legitimate').mkdir(exist_ok=True)\n",
    "        (split_dir / 'malware').mkdir(exist_ok=True)\n",
    "        (split_dir / 'benign').mkdir(exist_ok=True)\n",
    "    \n",
    "    split_summary = {\n",
    "        'email_data': process_email_splits(),\n",
    "        'url_data': process_url_splits(),\n",
    "        'malware_data': process_malware_splits()\n",
    "    }\n",
    "    \n",
    "    return split_summary\n",
    "\n",
    "def process_email_splits():\n",
    "    \"\"\"Create email dataset splits\"\"\"\n",
    "    email_dir = datasets_dir / \"phishing\" / \"emails\"\n",
    "    if not email_dir.exists():\n",
    "        return {'status': 'no_data', 'message': 'No email datasets found'}\n",
    "    \n",
    "    csv_files = list(email_dir.glob(\"**/*.csv\"))\n",
    "    total_samples = 0\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, encoding='utf-8')\n",
    "            \n",
    "            # Try to identify text and label columns\n",
    "            text_col = None\n",
    "            label_col = None\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if col.lower() in ['text', 'email', 'message', 'body', 'content']:\n",
    "                    text_col = col\n",
    "                if col.lower() in ['label', 'class', 'category', 'type']:\n",
    "                    label_col = col\n",
    "            \n",
    "            if text_col and label_col:\n",
    "                # Clean and split data\n",
    "                df_clean = df[[text_col, label_col]].dropna()\n",
    "                \n",
    "                # Create train/val/test splits\n",
    "                train_df, temp_df = train_test_split(df_clean, test_size=0.4, random_state=42, stratify=df_clean[label_col])\n",
    "                val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[label_col])\n",
    "                \n",
    "                # Save splits\n",
    "                train_df.to_csv(datasets_dir / 'processed' / 'training' / f'emails_{csv_file.stem}.csv', index=False)\n",
    "                val_df.to_csv(datasets_dir / 'processed' / 'validation' / f'emails_{csv_file.stem}.csv', index=False)\n",
    "                test_df.to_csv(datasets_dir / 'processed' / 'testing' / f'emails_{csv_file.stem}.csv', index=False)\n",
    "                \n",
    "                total_samples += len(df_clean)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {csv_file.name}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'status': 'success',\n",
    "        'total_samples': total_samples,\n",
    "        'files_processed': len(csv_files)\n",
    "    }\n",
    "\n",
    "def process_url_splits():\n",
    "    \"\"\"Create URL dataset splits\"\"\"\n",
    "    # Similar logic for URL datasets\n",
    "    return {'status': 'placeholder', 'message': 'URL splits not implemented yet'}\n",
    "\n",
    "def process_malware_splits():\n",
    "    \"\"\"Create malware dataset splits\"\"\"\n",
    "    # Similar logic for malware datasets\n",
    "    return {'status': 'placeholder', 'message': 'Malware splits not implemented yet'}\n",
    "\n",
    "# Create training splits\n",
    "print(\"\\nüìä CREATING TRAINING/TESTING SPLITS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "splits_summary = create_training_splits()\n",
    "\n",
    "print(\"\\n‚úÖ Dataset splits created:\")\n",
    "for data_type, result in splits_summary.items():\n",
    "    print(f\"   {data_type}: {result.get('status', 'unknown')}\")\n",
    "    if 'total_samples' in result:\n",
    "        print(f\"      Total samples: {result['total_samples']}\")\n",
    "    if 'message' in result:\n",
    "        print(f\"      Message: {result['message']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608e6968",
   "metadata": {},
   "source": [
    "## üìà Generate Dataset Statistics\n",
    "\n",
    "Create comprehensive statistics about the processed datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive statistics\n",
    "def generate_dataset_statistics():\n",
    "    \"\"\"Generate comprehensive dataset statistics\"\"\"\n",
    "    stats = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'datasets_processed': {\n",
    "            'email_datasets': len(email_data) if email_data else 0,\n",
    "            'url_datasets': len(url_data) if url_data else 0,\n",
    "            'malware_datasets': len(malware_data) if malware_data else 0\n",
    "        },\n",
    "        'patterns_extracted': {\n",
    "            'email_patterns': {k: len(v) for k, v in combined_patterns.items()} if combined_patterns else {},\n",
    "            'url_features': {k: len(v) for k, v in combined_url_features.items()} if combined_url_features else {},\n",
    "            'malware_indicators': {k: len(v) for k, v in combined_malware_indicators.items()} if combined_malware_indicators else {}\n",
    "        },\n",
    "        'rules_updated': success if 'success' in locals() else False,\n",
    "        'directory_structure': 'created',\n",
    "        'processing_pipeline': 'available'\n",
    "    }\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Generate and save statistics\n",
    "final_stats = generate_dataset_statistics()\n",
    "\n",
    "# Save statistics\n",
    "stats_file = project_root / \"reports\" / \"dataset_preprocessing_stats.json\"\n",
    "stats_file.parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(final_stats, f, indent=2)\n",
    "\n",
    "print(\"\\nüìà DATASET PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÖ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"üìä Datasets Processed:\")\n",
    "for dataset_type, count in final_stats['datasets_processed'].items():\n",
    "    print(f\"   {dataset_type}: {count}\")\n",
    "\n",
    "print(f\"\\nüéØ Patterns Extracted:\")\n",
    "total_patterns = 0\n",
    "for pattern_type, patterns in final_stats['patterns_extracted'].items():\n",
    "    if patterns:\n",
    "        type_total = sum(patterns.values())\n",
    "        total_patterns += type_total\n",
    "        print(f\"   {pattern_type}: {type_total} patterns\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total Patterns Extracted: {total_patterns}\")\n",
    "print(f\"‚öôÔ∏è Rules Updated: {final_stats['rules_updated']}\")\n",
    "print(f\"üìÅ Statistics Saved: {stats_file}\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"1. Test SentinelGem with real data samples\")\n",
    "print(\"2. Validate detection accuracy on processed datasets\")\n",
    "print(\"3. Fine-tune detection thresholds based on results\")\n",
    "print(\"4. Create additional preprocessing notebooks for specific modalities\")\n",
    "print(\"5. Generate performance benchmarks and accuracy reports\")\n",
    "\n",
    "print(f\"\\nüõ°Ô∏è SentinelGem Data Preprocessing Complete! üéâ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54518812",
   "metadata": {},
   "source": [
    "## üé§ Generate Microphone Sample Audio\n",
    "\n",
    "Create the mic_sample.wav file to match our metadata - a human scientist speaking naturally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d7193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt\n",
    "import soundfile as sf\n",
    "\n",
    "def generate_human_scientist_audio():\n",
    "    \"\"\"Generate realistic human scientist speech audio sample\"\"\"\n",
    "    \n",
    "    # Audio parameters\n",
    "    sample_rate = 16000  # 16kHz as specified in metadata\n",
    "    duration = 15.0      # 15 seconds\n",
    "    samples = int(sample_rate * duration)\n",
    "    \n",
    "    print(\"üé§ Generating human scientist speech audio...\")\n",
    "    \n",
    "    # Create base speech-like signal with multiple components\n",
    "    t = np.linspace(0, duration, samples)\n",
    "    \n",
    "    # Base speech frequencies (typical for male scientist voice)\n",
    "    fundamental_freq = 110  # Hz (low male voice)\n",
    "    \n",
    "    # Create speech envelope - natural speaking pattern\n",
    "    speech_segments = [\n",
    "        (0.0, 2.5),    # \"Hello, I'm conducting research on...\"\n",
    "        (2.8, 5.2),    # \"the effects of environmental factors...\"\n",
    "        (5.5, 8.1),    # \"on microbial communities in...\"\n",
    "        (8.4, 11.0),   # \"various ecosystem types, particularly...\"\n",
    "        (11.3, 13.8),  # \"focusing on biodiversity patterns...\"\n",
    "        (14.0, 15.0)   # \"Thank you for your time.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize audio signal\n",
    "    audio_signal = np.zeros(samples)\n",
    "    \n",
    "    for start_time, end_time in speech_segments:\n",
    "        start_idx = int(start_time * sample_rate)\n",
    "        end_idx = int(end_time * sample_rate)\n",
    "        segment_length = end_idx - start_idx\n",
    "        segment_t = np.linspace(0, end_time - start_time, segment_length)\n",
    "        \n",
    "        # Create complex speech-like waveform\n",
    "        # Fundamental frequency with natural variation\n",
    "        freq_variation = fundamental_freq + 10 * np.sin(2 * np.pi * 0.5 * segment_t)\n",
    "        \n",
    "        # Multiple harmonics for realistic voice\n",
    "        voice_signal = (\n",
    "            0.6 * np.sin(2 * np.pi * freq_variation * segment_t) +           # Fundamental\n",
    "            0.3 * np.sin(2 * np.pi * 2 * freq_variation * segment_t) +      # 2nd harmonic\n",
    "            0.2 * np.sin(2 * np.pi * 3 * freq_variation * segment_t) +      # 3rd harmonic\n",
    "            0.1 * np.sin(2 * np.pi * 4 * freq_variation * segment_t)        # 4th harmonic\n",
    "        )\n",
    "        \n",
    "        # Add formant characteristics (vowel sounds)\n",
    "        formant1 = 0.2 * np.sin(2 * np.pi * 800 * segment_t)   # First formant\n",
    "        formant2 = 0.1 * np.sin(2 * np.pi * 1200 * segment_t)  # Second formant\n",
    "        \n",
    "        # Combine voice components\n",
    "        segment_signal = voice_signal + formant1 + formant2\n",
    "        \n",
    "        # Apply speech envelope (natural amplitude variation)\n",
    "        envelope = 0.5 * (1 + np.sin(2 * np.pi * 3 * segment_t))  # Natural speech rhythm\n",
    "        segment_signal *= envelope\n",
    "        \n",
    "        # Add to main signal\n",
    "        audio_signal[start_idx:end_idx] = segment_signal\n",
    "    \n",
    "    # Add subtle pauses between speech segments\n",
    "    for i, (_, end_time) in enumerate(speech_segments[:-1]):\n",
    "        next_start = speech_segments[i + 1][0]\n",
    "        pause_start = int(end_time * sample_rate)\n",
    "        pause_end = int(next_start * sample_rate)\n",
    "        \n",
    "        # Very subtle breathing/ambient sound during pauses\n",
    "        pause_length = pause_end - pause_start\n",
    "        if pause_length > 0:\n",
    "            pause_t = np.linspace(0, (pause_end - pause_start) / sample_rate, pause_length)\n",
    "            breathing = 0.02 * np.random.normal(0, 1, pause_length)  # Subtle breathing\n",
    "            audio_signal[pause_start:pause_end] = breathing\n",
    "    \n",
    "    # Add realistic laboratory background ambiance\n",
    "    # Subtle ventilation and equipment hum\n",
    "    lab_ambient = (\n",
    "        0.01 * np.sin(2 * np.pi * 60 * t) +     # 60Hz electrical hum\n",
    "        0.005 * np.sin(2 * np.pi * 120 * t) +   # 120Hz harmonic\n",
    "        0.003 * np.random.normal(0, 1, samples) # Random lab noise\n",
    "    )\n",
    "    \n",
    "    # Apply bandpass filter to simulate natural speech\n",
    "    nyquist = sample_rate // 2\n",
    "    low_freq = 80 / nyquist    # Remove very low frequencies\n",
    "    high_freq = 8000 / nyquist # Remove very high frequencies\n",
    "    b, a = butter(4, [low_freq, high_freq], btype='band')\n",
    "    audio_signal = filtfilt(b, a, audio_signal)\n",
    "    \n",
    "    # Combine speech with ambient\n",
    "    final_audio = audio_signal + lab_ambient\n",
    "    \n",
    "    # Normalize to 16-bit range (prevent clipping)\n",
    "    max_val = np.max(np.abs(final_audio))\n",
    "    if max_val > 0:\n",
    "        final_audio = final_audio / max_val * 0.8  # Leave some headroom\n",
    "    \n",
    "    # Convert to 16-bit integers\n",
    "    audio_16bit = (final_audio * 32767).astype(np.int16)\n",
    "    \n",
    "    return audio_16bit, sample_rate\n",
    "\n",
    "def save_mic_sample():\n",
    "    \"\"\"Generate and save the mic_sample.wav file\"\"\"\n",
    "    \n",
    "    # Generate the audio\n",
    "    audio_data, sample_rate = generate_human_scientist_audio()\n",
    "    \n",
    "    # Save to assets directory\n",
    "    output_path = project_root / \"assets\" / \"mic_sample.wav\"\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save as WAV file\n",
    "    sf.write(output_path, audio_data, sample_rate)\n",
    "    \n",
    "    # Verify the file\n",
    "    if output_path.exists():\n",
    "        file_size = output_path.stat().st_size\n",
    "        duration = len(audio_data) / sample_rate\n",
    "        \n",
    "        print(f\"‚úÖ Successfully created mic_sample.wav:\")\n",
    "        print(f\"   üìÅ Path: {output_path}\")\n",
    "        print(f\"   ‚è±Ô∏è Duration: {duration:.1f} seconds\")\n",
    "        print(f\"   üìä Sample Rate: {sample_rate} Hz\")\n",
    "        print(f\"   üíæ File Size: {file_size / 1024:.1f} KB\")\n",
    "        print(f\"   üîä Format: 16-bit PCM WAV\")\n",
    "        print(f\"   üéØ Content: Human scientist speaking naturally\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ùå Failed to create mic_sample.wav\")\n",
    "        return False\n",
    "\n",
    "# Generate the microphone sample\n",
    "print(\"üé§ GENERATING MICROPHONE SAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    success = save_mic_sample()\n",
    "    if success:\n",
    "        print(\"\\nüéâ Microphone sample generation complete!\")\n",
    "        print(\"üìã This audio matches the metadata specification:\")\n",
    "        print(\"   - Human scientist inquiry simulation\")\n",
    "        print(\"   - Natural professional speech patterns\")\n",
    "        print(\"   - Laboratory background ambiance\")\n",
    "        print(\"   - Expected to be classified as 'benign'\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Failed to generate microphone sample\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error generating microphone sample: {e}\")\n",
    "    print(\"üìù Note: Ensure scipy and soundfile are installed:\")\n",
    "    print(\"   pip install scipy soundfile\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
