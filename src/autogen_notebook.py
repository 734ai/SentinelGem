"""
SentinelGem Notebook Auto-Generator
Author: Muzan Sano

Generates Jupyter notebooks with threat analysis results and insights
"""

import os
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import nbformat as nbf
from nbformat.v4 import new_notebook, new_markdown_cell, new_code_cell

from rich.console import Console
from rich.progress import track

from .utils import ensure_directory, create_timestamp_filename, logger

console = Console()

class NotebookGenerator:
    """
    Generates analysis notebooks automatically from threat detection results
    """
    
    def __init__(
        self,
        output_dir: str = "./notebooks/autogen",
        template_dir: str = "./notebooks/templates"
    ):
        self.output_dir = Path(output_dir)
        self.template_dir = Path(template_dir)
        
        # Ensure output directory exists
        ensure_directory(str(self.output_dir))
        
        # Notebook templates
        self.templates = self._load_templates()
        
        logger.info(f"Notebook generator initialized - Output: {self.output_dir}")
    
    def _load_templates(self) -> Dict[str, str]:
        """Load notebook cell templates"""
        return {
            "header": """# ðŸ›¡ï¸ SentinelGem Threat Analysis Report

**Generated by:** SentinelGem AI Agent  
**Author:** Muzan Sano  
**Session ID:** {session_id}  
**Timestamp:** {timestamp}  
**Analysis Type:** {input_type} Analysis

---

## Executive Summary

This notebook contains automated threat analysis results from SentinelGem's multimodal AI system.
""",
            
            "threat_overview": """## ðŸš¨ Threat Detection Overview

**Input File:** `{input_file}`  
**File Type:** {input_type}  
**Analysis Timestamp:** {analysis_time}

### Detection Results
- **Threat Detected:** {threat_detected}
- **Confidence Score:** {confidence_score:.2%}
- **Threat Classification:** {threat_type}
- **Risk Level:** {risk_level}
""",
            
            "technical_analysis": """## ðŸ” Technical Analysis

### AI Model Analysis
The analysis was performed using Google's **Gemma 3n** model with the following configuration:

{ai_analysis}

### Detection Methodology
- **Primary Analysis:** Gemma 3n multimodal reasoning
- **Pattern Matching:** Rule-based threat indicators
- **Confidence Scoring:** Combined AI + pattern matching
""",
            
            "recommendations": """## ðŸ“‹ Security Recommendations

Based on the analysis results, the following actions are recommended:

{recommendations_list}

### Immediate Actions
{immediate_actions}

### Long-term Security Measures
{longterm_measures}
""",
            
            "metadata_analysis": """## ðŸ“Š Analysis Metadata

### Processing Details
```json
{metadata_json}
```

### Detection Patterns
{pattern_analysis}
""",
            
            "code_reproduction": """## ðŸ”¬ Analysis Reproduction

The following code can be used to reproduce this analysis:

```python
# SentinelGem Analysis Reproduction
from sentinelgem import SentinelAgent

# Initialize agent
agent = SentinelAgent()

# Perform analysis
result = agent.analyze_input(
    input_file="{input_file}",
    input_type="{input_type}"
)

# Display results
print(f"Threat Detected: {{result.threat_detected}}")
print(f"Confidence: {{result.confidence_score:.2%}}")
print(f"Type: {{result.threat_type}}")
```
""",
            
            "appendix": """## ðŸ“š Appendix

### Model Information
- **Primary Model:** Google Gemma 3n
- **OCR Engine:** Tesseract (for screenshot analysis)
- **Speech Recognition:** OpenAI Whisper (for audio analysis)
- **Framework:** PyTorch + Transformers

### Data Processing Pipeline
1. **Input Validation:** File type and size verification
2. **Preprocessing:** Format-specific preparation
3. **Feature Extraction:** Multimodal content analysis
4. **AI Inference:** Gemma 3n threat assessment
5. **Pattern Matching:** Rule-based validation
6. **Result Synthesis:** Combined scoring and recommendations

### References
- [Gemma 3n Model Card](https://huggingface.co/google/gemma-2-2b-it)
- [MITRE ATT&CK Framework](https://attack.mitre.org/)
- [NIST Cybersecurity Framework](https://www.nist.gov/cyberframework)

---
*This report was generated automatically by SentinelGem v1.0*  
*For questions or support, contact: Muzan Sano*
"""
        }
    
    def generate_analysis_notebook(self, analysis_record: Dict[str, Any]) -> str:
        """
        Generate complete analysis notebook from analysis record
        
        Args:
            analysis_record: Dictionary containing analysis results
            
        Returns:
            Path to generated notebook
        """
        try:
            result = analysis_record["result"]
            input_file = analysis_record["input_file"]
            input_type = analysis_record["input_type"]
            timestamp = analysis_record["timestamp"]
            session_id = analysis_record.get("session_id", "unknown")
            
            # Create notebook
            nb = new_notebook()
            
            # Header cell
            header_content = self.templates["header"].format(
                session_id=session_id,
                timestamp=timestamp.strftime("%Y-%m-%d %H:%M:%S UTC"),
                input_type=input_type.capitalize()
            )
            nb.cells.append(new_markdown_cell(header_content))
            
            # Threat overview
            risk_level = self._calculate_risk_level(result.confidence_score, result.threat_detected)
            overview_content = self.templates["threat_overview"].format(
                input_file=Path(input_file).name,
                input_type=input_type,
                analysis_time=timestamp.strftime("%Y-%m-%d %H:%M:%S"),
                threat_detected="âš ï¸ YES" if result.threat_detected else "âœ… NO",
                confidence_score=result.confidence_score,
                threat_type=result.threat_type,
                risk_level=risk_level
            )
            nb.cells.append(new_markdown_cell(overview_content))
            
            # Visualization code cell
            viz_code = self._generate_visualization_code(result, input_type)
            nb.cells.append(new_code_cell(viz_code))
            
            # Technical analysis
            ai_analysis_text = self._format_ai_analysis(result)
            technical_content = self.templates["technical_analysis"].format(
                ai_analysis=ai_analysis_text
            )
            nb.cells.append(new_markdown_cell(technical_content))
            
            # Recommendations
            if result.recommendations:
                recommendations_list = "\n".join(f"- {rec}" for rec in result.recommendations)
                immediate_actions = self._extract_immediate_actions(result.recommendations)
                longterm_measures = self._extract_longterm_measures(result.recommendations)
                
                rec_content = self.templates["recommendations"].format(
                    recommendations_list=recommendations_list,
                    immediate_actions=immediate_actions,
                    longterm_measures=longterm_measures
                )
                nb.cells.append(new_markdown_cell(rec_content))
            
            # Metadata analysis
            if result.metadata:
                metadata_json = json.dumps(
                    self._sanitize_metadata(result.metadata), 
                    indent=2, 
                    default=str
                )
                pattern_analysis = self._format_pattern_analysis(result.metadata)
                
                metadata_content = self.templates["metadata_analysis"].format(
                    metadata_json=metadata_json,
                    pattern_analysis=pattern_analysis
                )
                nb.cells.append(new_markdown_cell(metadata_content))
            
            # Code reproduction
            code_content = self.templates["code_reproduction"].format(
                input_file=input_file,
                input_type=input_type
            )
            nb.cells.append(new_markdown_cell(code_content))
            
            # Raw analysis data (if available)
            if result.raw_analysis:
                raw_analysis_cell = f"""## ðŸ¤– Raw AI Analysis Output

```
{result.raw_analysis[:2000]}  # Truncated for readability
```
"""
                nb.cells.append(new_markdown_cell(raw_analysis_cell))
            
            # Appendix
            nb.cells.append(new_markdown_cell(self.templates["appendix"]))
            
            # Generate filename
            filename = create_timestamp_filename(
                prefix=f"{input_type}_analysis",
                suffix=session_id[:8],
                extension="ipynb"
            )
            
            # Save notebook
            notebook_path = self.output_dir / filename
            with open(notebook_path, 'w', encoding='utf-8') as f:
                nbf.write(nb, f)
            
            logger.info(f"Generated analysis notebook: {notebook_path}")
            return str(notebook_path)
            
        except Exception as e:
            logger.error(f"Notebook generation failed: {e}")
            raise
    
    def _calculate_risk_level(self, confidence: float, threat_detected: bool) -> str:
        """Calculate risk level based on confidence and detection"""
        if not threat_detected:
            return "ðŸŸ¢ LOW"
        elif confidence < 0.5:
            return "ðŸŸ¡ MEDIUM"
        elif confidence < 0.8:
            return "ðŸŸ  HIGH"
        else:
            return "ðŸ”´ CRITICAL"
    
    def _generate_visualization_code(self, result, input_type: str) -> str:
        """Generate visualization code for the analysis"""
        base_code = '''# Analysis Visualization
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

# Create figure with subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Confidence visualization
confidence = {confidence}
threat_status = "{threat_status}"
colors = ['red' if threat_detected else 'green' for threat_detected in [{threat_detected}]]

ax1.bar(['Threat Confidence'], [confidence], color=colors[0], alpha=0.7)
ax1.set_ylim(0, 1)
ax1.set_ylabel('Confidence Score')
ax1.set_title(f'{{input_type}} Analysis: {{threat_status}}')
ax1.axhline(y=0.7, color='orange', linestyle='--', alpha=0.7, label='Threshold')
ax1.legend()

# Add confidence percentage text
ax1.text(0, confidence + 0.05, f'{{confidence:.1%}}', 
         ha='center', va='bottom', fontweight='bold', fontsize=12)

'''.format(
            confidence=result.confidence_score,
            threat_status="THREAT DETECTED" if result.threat_detected else "SAFE",
            threat_detected=result.threat_detected,
            input_type=input_type.capitalize()
        )
        
        # Add type-specific visualization
        if input_type == "screenshot" and result.metadata.get("pattern_analysis"):
            pattern_code = '''
# Pattern detection visualization
patterns = {pattern_data}
if patterns:
    categories = list(patterns.keys())
    counts = [len(patterns[cat]) for cat in categories]
    
    ax2.barh(categories, counts, color='orange', alpha=0.7)
    ax2.set_xlabel('Pattern Matches')
    ax2.set_title('Detected Phishing Patterns')
    
    # Add count labels
    for i, count in enumerate(counts):
        ax2.text(count + 0.1, i, str(count), va='center')
else:
    ax2.text(0.5, 0.5, 'No patterns detected', ha='center', va='center', transform=ax2.transAxes)
    ax2.set_title('Pattern Analysis')
'''.format(
                pattern_data=result.metadata.get("pattern_analysis", {}).get("detected_patterns", {})
            )
            base_code += pattern_code
        
        base_code += '''
plt.tight_layout()
plt.show()

# Analysis summary
print(f"Analysis completed at: {datetime.now()}")
print(f"Threat detected: {threat_detected}")
print(f"Confidence score: {confidence:.2%}")
print(f"Threat type: {threat_type}")
'''.format(
            threat_detected=result.threat_detected,
            confidence=result.confidence_score,
            threat_type=result.threat_type
        )
        
        return base_code
    
    def _format_ai_analysis(self, result) -> str:
        """Format AI analysis details"""
        ai_info = result.metadata.get("ai_analysis", {})
        
        return f"""
**Model Confidence:** {ai_info.get('confidence', result.confidence_score):.2%}  
**AI Threat Type:** {ai_info.get('threat_type', result.threat_type)}  
**Analysis Length:** {result.metadata.get('analysis_length', 'unknown')} characters  
**Model Path:** {result.metadata.get('model', 'Gemma 3n')}

**Description:** {result.description}
"""
    
    def _extract_immediate_actions(self, recommendations: List[str]) -> str:
        """Extract immediate action items from recommendations"""
        immediate_keywords = ["immediately", "now", "stop", "do not", "avoid", "disconnect"]
        immediate_actions = []
        
        for rec in recommendations:
            if any(keyword in rec.lower() for keyword in immediate_keywords):
                immediate_actions.append(f"- {rec}")
        
        return "\n".join(immediate_actions) if immediate_actions else "- Review all recommendations above"
    
    def _extract_longterm_measures(self, recommendations: List[str]) -> str:
        """Extract long-term security measures from recommendations"""
        longterm_keywords = ["regularly", "always", "implement", "update", "monitor", "training"]
        longterm_measures = []
        
        for rec in recommendations:
            if any(keyword in rec.lower() for keyword in longterm_keywords):
                longterm_measures.append(f"- {rec}")
        
        # Add default long-term measures
        if not longterm_measures:
            longterm_measures = [
                "- Implement regular security awareness training",
                "- Keep security software and systems updated",
                "- Establish incident response procedures",
                "- Monitor for similar threats regularly"
            ]
        
        return "\n".join(longterm_measures)
    
    def _sanitize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitize metadata for JSON serialization"""
        sanitized = {}
        
        for key, value in metadata.items():
            if isinstance(value, (str, int, float, bool, list)):
                sanitized[key] = value
            elif isinstance(value, dict):
                sanitized[key] = self._sanitize_metadata(value)
            else:
                sanitized[key] = str(value)
        
        return sanitized
    
    def _format_pattern_analysis(self, metadata: Dict[str, Any]) -> str:
        """Format pattern analysis information"""
        pattern_info = metadata.get("pattern_analysis", {})
        
        if not pattern_info:
            return "No pattern analysis data available."
        
        output = []
        
        # Pattern detection score
        if "phishing_score" in pattern_info:
            output.append(f"**Pattern Detection Score:** {pattern_info['phishing_score']:.2%}")
        
        # Detected patterns
        detected_patterns = pattern_info.get("detected_patterns", {})
        if detected_patterns:
            output.append("\n**Detected Pattern Categories:**")
            for category, patterns in detected_patterns.items():
                output.append(f"- **{category.replace('_', ' ').title()}:** {len(patterns)} matches")
                for pattern in patterns[:3]:  # Show first 3 matches
                    output.append(f"  - \"{pattern}\"")
                if len(patterns) > 3:
                    output.append(f"  - ... and {len(patterns) - 3} more")
        
        # URLs analysis
        if "suspicious_urls" in pattern_info:
            suspicious_urls = pattern_info["suspicious_urls"]
            if suspicious_urls:
                output.append(f"\n**Suspicious URLs Found:** {len(suspicious_urls)}")
                for url in suspicious_urls[:3]:  # Show first 3 URLs
                    output.append(f"- `{url}`")
        
        return "\n".join(output) if output else "No significant patterns detected."
    
    def generate_session_summary_notebook(
        self, 
        session_data: Dict[str, Any], 
        analysis_history: List[Dict[str, Any]]
    ) -> str:
        """
        Generate session summary notebook
        
        Args:
            session_data: Session statistics
            analysis_history: List of all analyses in session
            
        Returns:
            Path to generated summary notebook
        """
        try:
            # Create notebook
            nb = new_notebook()
            
            # Session header
            session_header = f"""# ðŸ“Š SentinelGem Session Summary

**Session ID:** {session_data['session_id']}  
**Total Analyses:** {session_data['total_analyses']}  
**Threats Detected:** {session_data['threats_detected']}  
**Threat Detection Rate:** {session_data['threat_rate']:.1%}  
**Duration:** {session_data.get('start_time', 'Unknown')} - {session_data.get('end_time', 'Unknown')}

---
"""
            nb.cells.append(new_markdown_cell(session_header))
            
            # Session statistics visualization
            stats_code = f'''
# Session Statistics Visualization
import matplotlib.pyplot as plt
import numpy as np

# Session data
total_analyses = {session_data['total_analyses']}
threats_detected = {session_data['threats_detected']}
safe_analyses = total_analyses - threats_detected

# Create summary chart
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# Threat detection pie chart
labels = ['Safe', 'Threats']
sizes = [safe_analyses, threats_detected]
colors = ['green', 'red']
explode = (0, 0.1)  # explode threat slice

ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',
        shadow=True, startangle=90)
ax1.set_title('Threat Detection Overview')

# Input type analysis
input_types = {dict(session_data.get('input_type_counts', {}))}
if input_types:
    types = list(input_types.keys())
    counts = list(input_types.values())
    
    ax2.bar(types, counts, color=['blue', 'orange', 'purple', 'cyan'][:len(types)])
    ax2.set_xlabel('Input Type')
    ax2.set_ylabel('Count')
    ax2.set_title('Analysis by Input Type')
    ax2.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

print(f"Session completed with {{total_analyses}} analyses")
print(f"Threat detection rate: {{threats_detected/total_analyses:.1%}}")
'''
            nb.cells.append(new_code_cell(stats_code))
            
            # Analysis history table
            if analysis_history:
                history_md = "## ðŸ“‹ Analysis History\n\n"
                history_md += "| Time | File | Type | Threat | Confidence | Threat Type |\n"
                history_md += "|------|------|------|--------|------------|-------------|\n"
                
                for record in analysis_history[-20:]:  # Last 20 analyses
                    result = record["result"]
                    history_md += f"| {record['timestamp'].strftime('%H:%M:%S')} | "
                    history_md += f"{Path(record['input_file']).name} | "
                    history_md += f"{record['input_type']} | "
                    history_md += f"{'âš ï¸' if result.threat_detected else 'âœ…'} | "
                    history_md += f"{result.confidence_score:.1%} | "
                    history_md += f"{result.threat_type} |\n"
                
                nb.cells.append(new_markdown_cell(history_md))
            
            # Generate filename
            filename = create_timestamp_filename(
                prefix="session_summary",
                suffix=session_data['session_id'][:8],
                extension="ipynb"
            )
            
            # Save notebook
            notebook_path = self.output_dir / filename
            with open(notebook_path, 'w', encoding='utf-8') as f:
                nbf.write(nb, f)
            
            logger.info(f"Generated session summary notebook: {notebook_path}")
            return str(notebook_path)
            
        except Exception as e:
            logger.error(f"Session summary notebook generation failed: {e}")
            raise

# Global notebook generator instance
_notebook_generator = None

def get_notebook_generator(**kwargs) -> NotebookGenerator:
    """Get or create global notebook generator instance"""
    global _notebook_generator
    
    if _notebook_generator is None:
        _notebook_generator = NotebookGenerator(**kwargs)
    
    return _notebook_generator

if __name__ == "__main__":
    # Test notebook generator
    console.print("[bold cyan]Testing SentinelGem Notebook Generator[/bold cyan]")
    
    # Initialize generator
    generator = NotebookGenerator()
    
    console.print(f"[green]Notebook generator ready - Output: {generator.output_dir}[/green]")
